# Basic experiment info
experiment_name: gsm8k-test
trial_name: my-trial
seed: 1
mode: ray
wandb:
  mode: disabled
  entity: null
  project: null
  name: null
  job_type: null
  group: null
  notes: null
  tags: null
  config: null
tensorboard:
  path: null

exp_ctrl:
  total_train_epochs: 5
  save_freq_epochs: 1
  save_freq_steps: null
  save_freq_secs: null
  ckpt_freq_epochs: null
  ckpt_freq_steps: null
  ckpt_freq_secs: 600
  eval_freq_epochs: null
  eval_freq_steps: null
  eval_freq_secs: null
  benchmark_steps: null
  benchmark_n_seqs: null

# whether to allow persistent servers
shutdown_server_on_exit: false

# Allocation and parallelism
allocation_mode: sglang.d4p1m1+d2p2m1
n_nodes: 1
n_gpus_per_node: 8

# Cluster configuration
ray_temp_path: /tmp/ray
cluster:
  cluster_name: local
  fileroot: /tmp/arealite/
  n_nodes: 1
  n_gpus_per_node: 8
  name_resolve:
    type: nfs
    nfs_record_root: /tmp/arealite/name_resolve/

# Datasets
train_dataset:
  path: openai/gsm8k
  name: main
  split: train
  batch_size: 128
  shuffle: True
  preprocessor:
    type: gsm8k
    gsm8k:
      reward_mode: strict
valid_dataset: null

# Rollout config
rollout:
  collector:
    type: rlvr
    rlvr:
      reward_type: gsm8k
  num_workers: 1
  max_concurrent_rollouts: null
  max_head_offpolicyness: 4
  filter_reward_lb: -10000
  filter_reward_ub: 10000
  llm_client:
    server_backend: sglang
    schedule_policy: round_robin
    tokenizer_path: Qwen/Qwen2-0.5B-Instruct
    request_timeout: 3600
    request_retries: 3
  gconfig:
    n_samples: 16
    max_new_tokens: 512
    min_new_tokens: 0
    top_p: 1.0
    top_k: 1000000
    temperature: 1.0

# Trainer
trainer:
  type: grpo
  grpo:
    async_training: true
    actor:
      path: Qwen/Qwen2-0.5B-Instruct
      init_from_scratch: false
      gradient_checkpointing: false
      bf16: false
      optimizer:
        type: adam
        lr: 2.0e-05
        weight_decay: 0.05
        beta1: 0.9
        beta2: 0.95
        eps: 1.0e-05
        min_lr_ratio: 0.0
        lr_scheduler_type: constant
        warmup_steps_proportion: 0.001
        initial_loss_scale: 4294967296.0
        min_loss_scale: 1.0
        loss_scale_window: 5.0
        hysteresis: 2
        gradient_clipping: 1.0
      backend:
        type: hf
    ref: null
    mb_spec:
      max_tokens_per_mb: 10240
    # Algorithm
    group_adv_norm: False
    ppo_n_minibatches: 4
    eps_clip: 0.2
    c_clip: null
    reward_scaling: 10.0
    reward_bias: -0.5
    max_reward_clip: 20.0
    mask_no_eos_with_zero: false
    discount: 1.0
    gae_lambda: 1.0
    adv_norm: true
    kl_ctl: 0.0
    recompute_logprob: true
    use_decoupled_loss: true
    behav_imp_weight_cap: null

